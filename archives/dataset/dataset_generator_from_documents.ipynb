{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q openai\n",
    "# !pip install -q langchain\n",
    "# !pip install -q guardrails-ai\n",
    "# !pip install -q faiss-cpu\n",
    "# !pip install -q pypdf\n",
    "# !pip install -q python-dotenv\n",
    "# !pip install -q datasets\n",
    "# !pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage,  SystemMessage\n",
    "\n",
    "#Guardrails\n",
    "import openai\n",
    "from rich import print\n",
    "from langchain.output_parsers import GuardrailsOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Varible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "huggingface_api_key = os.environ.get(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset\n",
    "api = HfApi(token=huggingface_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFDirectoryLoader(\"/content/sample_data/Data/\")\n",
    "# loader = PyPDFDirectoryLoader(\"../cyber\")\n",
    "loader = PyPDFDirectoryLoader(\"../data\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">© <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span> . D\n",
       "r. Zhi Gang Sha  &amp; Rulin Xiu . This is a research/review paper, distributed under the ter\n",
       "</pre>\n"
      ],
      "text/plain": [
       "© \u001b[1;36m2020\u001b[0m . D\n",
       "r. Zhi Gang Sha  & Rulin Xiu . This is a research/review paper, distributed under the ter\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Data has been written to Extracted_data.txt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Data has been written to Extracted_data.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name = \"Extracted_data.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_name, \"w\",encoding=\"utf-8\") as file:\n",
    "    for item in data:\n",
    "        # Extract the \"page_content\" attribute from each item in the list\n",
    "        page_content = item.page_content\n",
    "        # Write the page_content to the file followed by a newline\n",
    "        file.write(page_content + \"\\n\")\n",
    "print(f\"Data has been written to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Lenth of the whole documentation is: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">364</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Lenth of the whole documentation is: \u001b[1;36m364\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 05: Split the Extracted Data into Text Chunks\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "print(\"Lenth of the whole documentation is:\",len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template\n",
    "\n",
    "[INPUT TEXT]\n",
    "\n",
    "[CONTEXT ]\n",
    "\n",
    "[REQUEST FOR Q+A]\n",
    "\n",
    "[RESPONSE SAMPLE]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Context of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.6) # for sythetic data generation\n",
    "model = OpenAI(temperature=0) # for parsing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This research paper proposes Information Mechanics as a framework to understand the process of <span style=\"color: #008000; text-decoration-color: #008000\">\"It from Bit,\"</span> where\n",
       "information determines observed phenomena. The interaction of information space and time creates everything we \n",
       "observe, and the paper introduces the information action and function to calculate the possibilities and states in \n",
       "a system. Keywords: information mechanics, <span style=\"color: #008000; text-decoration-color: #008000\">\"It from Bit,\"</span> hierarchy problem, cosmological constant problem, dark \n",
       "matter, dark energy, black hole, grand unification theory.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "This research paper proposes Information Mechanics as a framework to understand the process of \u001b[32m\"It from Bit,\"\u001b[0m where\n",
       "information determines observed phenomena. The interaction of information space and time creates everything we \n",
       "observe, and the paper introduces the information action and function to calculate the possibilities and states in \n",
       "a system. Keywords: information mechanics, \u001b[32m\"It from Bit,\"\u001b[0m hierarchy problem, cosmological constant problem, dark \n",
       "matter, dark energy, black hole, grand unification theory.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=f\"{data[0].page_content}give the above information give me a single line rich summary with keywords in the beginning which can be used to descibe the entiner inforamtion\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating QnA \n",
    "\n",
    "- *prompt* : provide {questions_per_chunk} question and answer pairs base on the text above , The Question must begin with \\n\"In the context of ...\\\".The answer borrow, verbatim, from the text above. In providing each question consider that the reader does not see of have access to any of the other questions from context. Vary the style and formate fo quesitons. Respond in plain test on a new line for each question and answer. Do not include Do no include qestion numbers, Here is an exmaple of two question and answer paids:\\n\\n {tain_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"TaoGPT-v1.json\"\n",
    "\n",
    "json_response_format = [\n",
    "                {\n",
    "                    \"question\": \"In the context of ...\",\n",
    "                    \"answer\": \"...\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"In the context of ...\",\n",
    "                    \"answer\": \"...\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"In the context of ...\",\n",
    "                    \"answer\": \"...\"\n",
    "                }\n",
    "            ]\n",
    "\n",
    "rail_spec = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "<output>\n",
    "<list name=\"data\" description=\"list of question answer pairs\">\n",
    "    <object>\n",
    "        <string name=\"question\" description=\"the question\"/>\n",
    "        <string name=\"answer\"  description=\"the answer\"/>\n",
    "    </object>\n",
    "</list>\n",
    "</output>\n",
    "<prompt>\n",
    "\n",
    "Given the following list of json question and answer paids , please extract it in a proper JSON formate \n",
    "${generated_data_json}\n",
    "\n",
    "${gr.complete_json_suffix_v2}\n",
    "</prompt>\n",
    "</rail>\n",
    "\"\"\"\n",
    "def write_to_json_file(json_data, json_file_name):\n",
    "    try:\n",
    "        try:\n",
    "            with open(json_file_name, \"r\") as outfile:\n",
    "                data = json.load(outfile)\n",
    "            for json_data_pairs in json_data:\n",
    "                data.append(json_data_pairs)\n",
    "            with open(json_file_name, \"w\") as outfile:\n",
    "                json.dump(data, outfile)\n",
    "        except FileNotFoundError:\n",
    "            with open(json_file_name, \"w\") as outfile:\n",
    "                json.dump(json_data, outfile)        \n",
    "    except Exception as e:\n",
    "        print(\"Error in write_to_json_file\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_chunks:\n",
    "    try:\n",
    "        context_messages = [\n",
    "            HumanMessage(\n",
    "                content=f\"{text.page_content}give the above information give me a single line rich summary with keywords in the beginning which can be used to descibe the entiner inforamtion\"\n",
    "            ),\n",
    "        ]\n",
    "        context_response = chat(context_messages)\n",
    "        print(context_response.content)\n",
    "        try:\n",
    "            generated_messages = [\n",
    "                HumanMessage(\n",
    "                    content=f\"\"\"given the context:{context_response} \n",
    "                    and the information {text.page_content} provide 5 question and answer pairs base on the text above , \n",
    "                    The Question must begin with \n",
    "                    \"In the context of ...\\\".The answer borrow, verbatim, from the text above. In providing each question consider that the reader does not see of have access to any of the other questions from context. Vary the style and formate fo quesitons. \n",
    "                    Respond in only JSON following this formate\n",
    "                    {json_response_format} and nothing else\"\"\"\n",
    "                ),\n",
    "            ]\n",
    "            generated_response = chat(generated_messages)\n",
    "            try:\n",
    "                eval_generated_json = eval(generated_response.content)\n",
    "                if len(eval_generated_json) >= 1:\n",
    "                    write_to_json_file(eval_generated_json, dataset_name)\n",
    "                    print(eval_generated_json)\n",
    "                else:\n",
    "                    raise Exception(\"Data generated not in the right format\")\n",
    "            except:\n",
    "                # output_parser = GuardrailsOutputParser.from_rail_string(extractor_rail)\n",
    "                output_parser = GuardrailsOutputParser.from_rail_string(rail_spec, api=openai.ChatCompletion.create)\n",
    "                prompt = PromptTemplate(\n",
    "                    template=output_parser.guard.base_prompt,\n",
    "                    input_variables=output_parser.guard.prompt.variable_names,\n",
    "                )\n",
    "\n",
    "                model = OpenAI(temperature=0) # type: ignore\n",
    "                generated_response_final = model(prompt.format_prompt(generated_data_json=generated_response.content).to_string())\n",
    "                generated_response_final = output_parser.parse(generated_response_final)\n",
    "                generated_response_final = generated_response_final[\"data\"]\n",
    "                print(generated_response_final)                \n",
    "                write_to_json_file(generated_response_final, dataset_name)\n",
    "        except:\n",
    "            print(\"Error in generating the data\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to get context of the text\")\n",
    "    completion_precentage = text_chunks.index(text)/len(text_chunks)*100s\n",
    "    print(f\"\\n---------------------------------------------------{completion_precentage}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=\"./TaoGPT-v1.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(\"Dataset name\")\n",
    "dataset.push_to_hub(\"agency888/TaoGPT-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing Dataset.json\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./TaoGPT-v1.json\",\n",
    "    path_in_repo=\"TaoGPT-v1.json\",\n",
    "    repo_id=\"agency888/TaoGPT-v1\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing Dataset Readme\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=\"agency888/TaoGPT-v1\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from the JSON file\n",
    "with open(\"./TaoGPT-v1.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "error = 0\n",
    "# Format and add the new fields to each item in the data using a for loop\n",
    "formatted_data = []\n",
    "for item in data:\n",
    "    # formatted_item = format_and_add_fields(item)\n",
    "    # formatted_data.append(formatted_item)\n",
    "    # print(item)\n",
    "    try:\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        # Create text field\n",
    "        item[\"text\"] = f\"{question} {answer}\"\n",
    "\n",
    "        # Create text_finetuning field\n",
    "        item[\"text_finetuning\"] = f\"Here is a question based on Taoscience. ### Question : {question} answer in detail ### Answer : <s> {answer} </s>\"\n",
    "\n",
    "        # Create text_mistral field\n",
    "        item[\"text_mistral\"] = f\"<s>[INST] {question}[/INST]{answer}</s>\"\n",
    "        formatted_data.append(item)\n",
    "    except:\n",
    "        error += 1\n",
    "        print(f\"Error in formatting the data at {data.index(item)} index \\n\\n Take a look at this: \\n{item}\")\n",
    "# Save the updated data back to the JSON file\n",
    "with open(\"TaoGPT-v1-formatted.json\", \"w\") as file:\n",
    "    json.dump(formatted_data, file, indent=4)\n",
    "\n",
    "print(\"Formatted data has been saved to formatted_data.json\")\n",
    "print(\"Number of errors:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=\"./TaoGPT-v1-formatted.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(\"Dataset name\")\n",
    "dataset.push_to_hub(\"agency888/TaoGPT-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing Dataset.json\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./TaoGPT-v1-formatted.json\",\n",
    "    path_in_repo=\"TaoGPT-v1-formatted.json\",\n",
    "    repo_id=\"agency888/TaoGPT-v1\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
